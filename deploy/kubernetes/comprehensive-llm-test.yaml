apiVersion: v1
kind: ConfigMap
metadata:
  name: dfastllm-comprehensive-config
data:
  # Model configuration
  VDIFF_MODEL: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  VDIFF_MAX_MODEL_LEN: "2048"
  VDIFF_TRUST_REMOTE_CODE: "true"
  VDIFF_PORT: "8000"
  VDIFF_HOST: "0.0.0.0"
  
  # Hybrid Mode
  VDIFF_HYBRID_ENABLED: "true"
  VDIFF_HYBRID_MODE: "deer"
  VDIFF_HYBRID_DRAFT_SIZE: "8"
  VDIFF_HYBRID_ADAPTIVE: "true"
  
  # MoR
  VDIFF_MOR_ENABLED: "true"
  VDIFF_MOR_STRATEGY: "confidence"
  
  # Optimizations
  VDIFF_COMPILE: "true"
  VDIFF_FLASH_ATTENTION: "true"
  VDIFF_DIFFUSION_STEPS: "64"
  
  # Fix HuggingFace offline mode
  HF_HUB_OFFLINE: "0"
  TRANSFORMERS_OFFLINE: "0"
  HF_HUB_DISABLE_PROGRESS_BARS: "1"
  HF_HOME: "/tmp/hf_cache"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: dfastllm-comprehensive-test
  labels:
    app: dfastllm-comprehensive-test
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 3600
  template:
    metadata:
      labels:
        app: dfastllm-comprehensive-test
    spec:
      restartPolicy: Never
      containers:
      - name: comprehensive-test
        image: quay.io/modh/vllm:rhoai-2.20-cuda
        imagePullPolicy: Always
        resources:
          limits:
            nvidia.com/gpu: 1
          requests:
            nvidia.com/gpu: 1
        envFrom:
          - configMapRef:
              name: dfastllm-comprehensive-config
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘     COMPREHENSIVE LLM INFERENCE TEST SUITE                       â•‘"
          echo "â•‘     dfastllm Hybrid Diffusion-AR Engine                          â•‘"
          echo "â•‘     Date: $(date)                              â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          
          # ============================================================
          # PHASE 0: Environment Setup
          # ============================================================
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "PHASE 0: Environment Setup"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          
          echo "GPU Information:"
          nvidia-smi --query-gpu=name,memory.total,driver_version,temperature.gpu --format=csv
          echo ""
          
          echo "Installing dfastllm..."
          pip install --no-cache-dir --no-deps --target=/tmp/dfastllm_pkg git+https://github.com/mwaykole/vdiff.git@main 2>&1 | tail -5
          export PYTHONPATH="/tmp/dfastllm_pkg:$PYTHONPATH"
          echo "âœ… dfastllm installed"
          echo ""
          
          # ============================================================
          # Run comprehensive Python test suite
          # ============================================================
          python3 << 'COMPREHENSIVE_TEST'
          import sys
          import time
          import json
          import os
          import traceback
          from dataclasses import dataclass, field, asdict
          from typing import List, Dict, Any, Optional
          from datetime import datetime
          
          sys.path.insert(0, '/tmp/dfastllm_pkg')
          
          # ============================================================
          # Test Results Tracking
          # ============================================================
          @dataclass
          class TestResult:
              name: str
              category: str
              passed: bool
              duration_ms: float
              details: str = ""
              metrics: Dict[str, Any] = field(default_factory=dict)
          
          class TestSuite:
              def __init__(self):
                  self.results: List[TestResult] = []
                  self.start_time = time.time()
              
              def add_result(self, result: TestResult):
                  self.results.append(result)
                  status = "âœ… PASS" if result.passed else "âŒ FAIL"
                  print(f"  {status}: {result.name} ({result.duration_ms:.1f}ms)")
                  if result.details and not result.passed:
                      print(f"       â””â”€ {result.details[:100]}")
              
              def run_test(self, name: str, category: str, test_fn):
                  start = time.perf_counter()
                  try:
                      result = test_fn()
                      duration = (time.perf_counter() - start) * 1000
                      if isinstance(result, dict):
                          self.add_result(TestResult(name, category, True, duration, metrics=result))
                      else:
                          self.add_result(TestResult(name, category, True, duration))
                  except Exception as e:
                      duration = (time.perf_counter() - start) * 1000
                      self.add_result(TestResult(name, category, False, duration, str(e)))
              
              def summary(self) -> Dict[str, Any]:
                  total = len(self.results)
                  passed = sum(1 for r in self.results if r.passed)
                  failed = total - passed
                  
                  by_category = {}
                  for r in self.results:
                      if r.category not in by_category:
                          by_category[r.category] = {"passed": 0, "failed": 0}
                      if r.passed:
                          by_category[r.category]["passed"] += 1
                      else:
                          by_category[r.category]["failed"] += 1
                  
                  return {
                      "total": total,
                      "passed": passed,
                      "failed": failed,
                      "pass_rate": (passed / total * 100) if total > 0 else 0,
                      "duration_sec": time.time() - self.start_time,
                      "by_category": by_category
                  }
          
          suite = TestSuite()
          
          # ============================================================
          # PHASE 1: SOLID Architecture Tests
          # ============================================================
          print("")
          print("=" * 70)
          print("PHASE 1: SOLID Architecture & Base Classes")
          print("=" * 70)
          
          def test_base_stats():
              from dfastllm.engine.base import BaseStats
              stats = BaseStats()
              result = stats.to_dict()
              assert isinstance(result, dict)
              return {"type": "BaseStats", "to_dict_works": True}
          
          def test_base_config():
              from dfastllm.engine.base import BaseConfig
              config = BaseConfig()
              assert config is not None
              return {"type": "BaseConfig"}
          
          def test_base_cache():
              from dfastllm.engine.base import BaseCache
              cache = BaseCache(max_size=100)
              cache.put("key1", "value1")
              assert cache.get("key1") == "value1"
              return {"cache_size": len(cache._cache)}
          
          def test_entropy_computer():
              from dfastllm.engine.base import EntropyComputer
              import torch
              if torch.cuda.is_available():
                  logits = torch.randn(1, 10, 1000).cuda()
                  entropy = EntropyComputer.compute(logits)
                  assert entropy.shape == (1, 10)
                  return {"entropy_shape": list(entropy.shape), "device": "cuda"}
              return {"device": "cpu", "skipped": True}
          
          def test_liskov_substitution():
              from dfastllm.engine.base import BaseStats, BaseConfig, BaseCache
              from dfastllm.engine.hybrid_engine import HybridStats, HybridConfig
              from dfastllm.engine.continuous_batching import BatcherStats, BatcherConfig, PrefixCache
              from dfastllm.engine.entropy_controller import EntropyStats, EntropyConfig
              
              assert issubclass(HybridStats, BaseStats)
              assert issubclass(BatcherStats, BaseStats)
              assert issubclass(EntropyStats, BaseStats)
              assert issubclass(HybridConfig, BaseConfig)
              assert issubclass(BatcherConfig, BaseConfig)
              assert issubclass(EntropyConfig, BaseConfig)
              assert issubclass(PrefixCache, BaseCache)
              return {"all_inherit_correctly": True}
          
          suite.run_test("BaseStats.to_dict()", "SOLID", test_base_stats)
          suite.run_test("BaseConfig initialization", "SOLID", test_base_config)
          suite.run_test("LRUCache operations", "SOLID", test_base_cache)
          suite.run_test("EntropyComputer on GPU", "SOLID", test_entropy_computer)
          suite.run_test("Liskov Substitution Principle", "SOLID", test_liskov_substitution)
          
          # ============================================================
          # PHASE 2: Hybrid Engine Tests
          # ============================================================
          print("")
          print("=" * 70)
          print("PHASE 2: Hybrid Diffusion-AR Engine")
          print("=" * 70)
          
          def test_hybrid_stats():
              from dfastllm.engine.hybrid_engine import HybridStats
              stats = HybridStats()
              stats.update(drafted=100, accepted=78, diffusion_time=0.5, ar_time=0.2)
              assert stats.tokens_accepted == 78
              assert stats.tokens_rejected == 22
              rate = stats.draft_acceptance_rate
              assert 0.77 < rate < 0.79
              return {"acceptance_rate": f"{rate:.2%}"}
          
          def test_hybrid_config():
              from dfastllm.engine.hybrid_engine import HybridConfig, HybridMode
              config = HybridConfig(mode=HybridMode.DEER, draft_block_size=8)
              assert config.mode == HybridMode.DEER
              assert config.draft_block_size == 8
              return {"mode": config.mode.value, "draft_size": config.draft_block_size}
          
          def test_hybrid_modes():
              from dfastllm.engine.hybrid_engine import HybridMode
              modes = [HybridMode.DEER, HybridMode.SPEC_DIFF, HybridMode.SEMI_AR, HybridMode.ADAPTIVE]
              return {"modes": [m.value for m in modes]}
          
          def test_hybrid_stats_to_dict():
              from dfastllm.engine.hybrid_engine import HybridStats
              stats = HybridStats()
              stats.update(drafted=50, accepted=40, diffusion_time=0.1, ar_time=0.05)
              d = stats.to_dict()
              assert "tokens_accepted" in d
              assert "draft_acceptance_rate" in d
              return d
          
          suite.run_test("HybridStats tracking", "Hybrid", test_hybrid_stats)
          suite.run_test("HybridConfig modes", "Hybrid", test_hybrid_config)
          suite.run_test("All hybrid modes exist", "Hybrid", test_hybrid_modes)
          suite.run_test("HybridStats.to_dict()", "Hybrid", test_hybrid_stats_to_dict)
          
          # ============================================================
          # PHASE 3: Entropy Controller Tests
          # ============================================================
          print("")
          print("=" * 70)
          print("PHASE 3: Entropy-Adaptive Controller")
          print("=" * 70)
          
          def test_entropy_config():
              from dfastllm.engine.entropy_controller import EntropyConfig, AdaptationStrategy
              config = EntropyConfig(
                  strategy=AdaptationStrategy.COMBINED,
                  high_entropy_threshold=2.0,
                  low_entropy_threshold=0.5
              )
              assert config.strategy == AdaptationStrategy.COMBINED
              return {"strategy": config.strategy.value}
          
          def test_entropy_stats():
              from dfastllm.engine.entropy_controller import EntropyStats
              stats = EntropyStats()
              stats.total_predictions = 10
              stats.high_entropy_count = 2
              stats.low_entropy_count = 6
              stats.avg_entropy = 1.5
              result = stats.to_dict()
              return {"high_pct": result["high_entropy_pct"], "low_pct": result["low_entropy_pct"]}
          
          def test_entropy_strategies():
              from dfastllm.engine.entropy_controller import AdaptationStrategy
              strategies = [AdaptationStrategy.DRAFT_LENGTH, AdaptationStrategy.TEMPERATURE, 
                           AdaptationStrategy.COMBINED, AdaptationStrategy.DIFFUSION_STEPS]
              return {"strategies": [s.value for s in strategies]}
          
          suite.run_test("EntropyConfig creation", "Entropy", test_entropy_config)
          suite.run_test("EntropyStats metrics", "Entropy", test_entropy_stats)
          suite.run_test("All entropy strategies", "Entropy", test_entropy_strategies)
          
          # ============================================================
          # PHASE 4: Continuous Batching & Prefix Cache
          # ============================================================
          print("")
          print("=" * 70)
          print("PHASE 4: Continuous Batching & Prefix Cache")
          print("=" * 70)
          
          def test_prefix_cache_basic():
              from dfastllm.engine.continuous_batching import PrefixCache
              # min_prefix_length=2 so short tuples work
              cache = PrefixCache(max_cache_size=100, min_prefix_length=2)
              tokens = list(range(20))  # 20 tokens to pass min length
              cache.put(tokens, "cached_value")
              result = cache.get(tokens)
              assert result == "cached_value"
              return {"size": len(cache._cache)}
          
          def test_prefix_cache_eviction():
              from dfastllm.engine.continuous_batching import PrefixCache
              cache = PrefixCache(max_cache_size=3, min_prefix_length=2)
              cache.put(list(range(10)), "v1")
              cache.put(list(range(10, 20)), "v2")
              cache.put(list(range(20, 30)), "v3")
              cache.put(list(range(30, 40)), "v4")  # Should evict oldest
              assert len(cache._cache) <= 3
              return {"eviction_works": True}
          
          def test_prefix_cache_performance():
              from dfastllm.engine.continuous_batching import PrefixCache
              cache = PrefixCache(max_cache_size=1000, min_prefix_length=2)
              
              # PUT performance
              start = time.perf_counter()
              for i in range(100):
                  cache.put(list(range(i*20, (i+1)*20)), f"value_{i}")
              put_time = (time.perf_counter() - start) * 1000
              
              # GET performance
              start = time.perf_counter()
              for i in range(100):
                  cache.get(list(range(i*20, (i+1)*20)))
              get_time = (time.perf_counter() - start) * 1000
              
              stats = cache.get_stats()
              return {
                  "put_ms": f"{put_time:.2f}",
                  "get_ms": f"{get_time:.2f}",
                  "hit_rate": f"{stats['hit_rate']:.1%}"
              }
          
          def test_batcher_config():
              from dfastllm.engine.continuous_batching import BatcherConfig
              config = BatcherConfig(max_batch_size=32, max_tokens_per_batch=4096)
              assert config.max_batch_size == 32
              return {"max_batch": config.max_batch_size}
          
          suite.run_test("PrefixCache basic ops", "Batching", test_prefix_cache_basic)
          suite.run_test("PrefixCache LRU eviction", "Batching", test_prefix_cache_eviction)
          suite.run_test("PrefixCache performance", "Batching", test_prefix_cache_performance)
          suite.run_test("BatcherConfig creation", "Batching", test_batcher_config)
          
          # ============================================================
          # PHASE 5: Model Loading & Generation
          # ============================================================
          print("")
          print("=" * 70)
          print("PHASE 5: Model Loading & Generation")
          print("=" * 70)
          
          model = None
          tokenizer = None
          
          def test_tokenizer_loading():
              global tokenizer
              from transformers import AutoTokenizer
              tokenizer = AutoTokenizer.from_pretrained(
                  "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                  trust_remote_code=True
              )
              assert tokenizer is not None
              return {"vocab_size": tokenizer.vocab_size}
          
          def test_model_loading():
              global model
              import torch
              from transformers import AutoModelForCausalLM
              model = AutoModelForCausalLM.from_pretrained(
                  "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                  torch_dtype=torch.float16,
                  device_map="cuda",
                  trust_remote_code=True
              )
              params = sum(p.numel() for p in model.parameters()) / 1e9
              return {"params_B": f"{params:.2f}", "device": str(model.device)}
          
          suite.run_test("Tokenizer loading", "Model", test_tokenizer_loading)
          suite.run_test("Model loading", "Model", test_model_loading)
          
          # ============================================================
          # PHASE 6: Generation Functional Tests
          # ============================================================
          print("")
          print("=" * 70)
          print("PHASE 6: Generation Functional Tests")
          print("=" * 70)
          
          def generate_text(prompt, max_new_tokens=50, **kwargs):
              import torch
              inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
              with torch.no_grad():
                  outputs = model.generate(
                      **inputs,
                      max_new_tokens=max_new_tokens,
                      pad_token_id=tokenizer.eos_token_id,
                      **kwargs
                  )
              return tokenizer.decode(outputs[0], skip_special_tokens=True)
          
          def test_basic_generation():
              result = generate_text("Hello, my name is", max_new_tokens=20)
              assert len(result) > 20
              return {"output_len": len(result)}
          
          def test_temperature_zero():
              # Deterministic output
              result1 = generate_text("The capital of France is", max_new_tokens=10, temperature=0.01, do_sample=False)
              result2 = generate_text("The capital of France is", max_new_tokens=10, temperature=0.01, do_sample=False)
              assert result1 == result2
              return {"deterministic": True}
          
          def test_temperature_high():
              import torch
              torch.manual_seed(42)
              result = generate_text("Write a random word:", max_new_tokens=10, temperature=1.5, do_sample=True)
              assert len(result) > 0
              return {"output_len": len(result)}
          
          def test_max_tokens():
              result = generate_text("Count to ten:", max_new_tokens=5)
              tokens = tokenizer.encode(result)
              # Should not generate excessively more
              return {"token_count": len(tokens)}
          
          def test_empty_prompt():
              try:
                  result = generate_text("", max_new_tokens=10)
                  return {"handled": True}
              except Exception as e:
                  return {"handled": True, "error_type": type(e).__name__}
          
          def test_unicode_prompt():
              result = generate_text("Translate to English: ä½ å¥½ä¸–ç•Œ ğŸŒ", max_new_tokens=20)
              assert len(result) > 0
              return {"handled_unicode": True}
          
          def test_long_prompt():
              long_prompt = "This is a test. " * 100
              result = generate_text(long_prompt, max_new_tokens=10)
              return {"handled_long_prompt": True}
          
          suite.run_test("Basic generation", "Generation", test_basic_generation)
          suite.run_test("Temperature=0 (deterministic)", "Generation", test_temperature_zero)
          suite.run_test("Temperature=1.5 (creative)", "Generation", test_temperature_high)
          suite.run_test("Max tokens limit", "Generation", test_max_tokens)
          suite.run_test("Empty prompt handling", "Generation", test_empty_prompt)
          suite.run_test("Unicode/Emoji handling", "Generation", test_unicode_prompt)
          suite.run_test("Long prompt (1000+ chars)", "Generation", test_long_prompt)
          
          # ============================================================
          # PHASE 7: Performance Benchmarks
          # ============================================================
          print("")
          print("=" * 70)
          print("PHASE 7: Performance Benchmarks")
          print("=" * 70)
          
          def test_throughput_benchmark():
              import torch
              prompts = [
                  "Explain quantum computing in simple terms:",
                  "What is machine learning?",
                  "Write a short poem about the ocean:",
                  "Describe the process of photosynthesis:",
                  "What are the benefits of exercise?",
              ]
              
              total_tokens = 0
              total_time = 0
              results = []
              
              for prompt in prompts:
                  inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
                  input_len = inputs["input_ids"].shape[1]
                  
                  torch.cuda.synchronize()
                  start = time.perf_counter()
                  
                  with torch.no_grad():
                      outputs = model.generate(
                          **inputs,
                          max_new_tokens=50,
                          pad_token_id=tokenizer.eos_token_id,
                          do_sample=False
                      )
                  
                  torch.cuda.synchronize()
                  elapsed = time.perf_counter() - start
                  
                  output_len = outputs.shape[1] - input_len
                  total_tokens += output_len
                  total_time += elapsed
                  results.append({"tokens": output_len, "time": elapsed, "tps": output_len / elapsed})
              
              avg_tps = total_tokens / total_time
              return {
                  "total_tokens": total_tokens,
                  "total_time_sec": f"{total_time:.2f}",
                  "avg_throughput_tps": f"{avg_tps:.1f}",
                  "prompts_tested": len(prompts)
              }
          
          def test_latency_benchmark():
              import torch
              
              # Warm up
              generate_text("Warmup", max_new_tokens=5)
              
              latencies = []
              for _ in range(5):
                  inputs = tokenizer("Hello", return_tensors="pt").to("cuda")
                  
                  torch.cuda.synchronize()
                  start = time.perf_counter()
                  
                  with torch.no_grad():
                      outputs = model.generate(**inputs, max_new_tokens=1, pad_token_id=tokenizer.eos_token_id)
                  
                  torch.cuda.synchronize()
                  latencies.append((time.perf_counter() - start) * 1000)
              
              latencies.sort()
              return {
                  "ttft_p50_ms": f"{latencies[2]:.1f}",
                  "ttft_p95_ms": f"{latencies[4]:.1f}",
                  "ttft_min_ms": f"{latencies[0]:.1f}"
              }
          
          def test_memory_usage():
              import torch
              torch.cuda.empty_cache()
              torch.cuda.reset_peak_memory_stats()
              
              # Generate with larger output
              generate_text("Write a detailed explanation of:", max_new_tokens=100)
              
              peak_memory_gb = torch.cuda.max_memory_allocated() / (1024**3)
              current_memory_gb = torch.cuda.memory_allocated() / (1024**3)
              
              return {
                  "peak_memory_gb": f"{peak_memory_gb:.2f}",
                  "current_memory_gb": f"{current_memory_gb:.2f}"
              }
          
          suite.run_test("Throughput benchmark", "Performance", test_throughput_benchmark)
          suite.run_test("Latency (TTFT) benchmark", "Performance", test_latency_benchmark)
          suite.run_test("GPU memory usage", "Performance", test_memory_usage)
          
          # ============================================================
          # PHASE 8: Edge Cases & Robustness
          # ============================================================
          print("")
          print("=" * 70)
          print("PHASE 8: Edge Cases & Robustness")
          print("=" * 70)
          
          def test_special_tokens():
              result = generate_text("<|endoftext|>", max_new_tokens=10)
              return {"handled": True}
          
          def test_repeated_tokens():
              result = generate_text("a a a a a a a a a a", max_new_tokens=10)
              return {"handled": True}
          
          def test_code_prompt():
              result = generate_text("def fibonacci(n):\n    ", max_new_tokens=30)
              return {"generated_code": True, "len": len(result)}
          
          def test_math_prompt():
              result = generate_text("2 + 2 = ", max_new_tokens=5)
              return {"handled_math": True}
          
          def test_multilingual():
              prompts = [
                  ("French", "Bonjour, comment allez-vous?"),
                  ("Spanish", "Hola, Â¿cÃ³mo estÃ¡s?"),
                  ("German", "Guten Tag, wie geht es Ihnen?"),
              ]
              results = {}
              for lang, prompt in prompts:
                  result = generate_text(prompt, max_new_tokens=10)
                  results[lang] = len(result) > 0
              return results
          
          suite.run_test("Special tokens handling", "Edge Cases", test_special_tokens)
          suite.run_test("Repeated tokens", "Edge Cases", test_repeated_tokens)
          suite.run_test("Code generation", "Edge Cases", test_code_prompt)
          suite.run_test("Math prompt", "Edge Cases", test_math_prompt)
          suite.run_test("Multilingual prompts", "Edge Cases", test_multilingual)
          
          # ============================================================
          # PHASE 9: MoR (Mixture of Recursions) Tests
          # ============================================================
          print("")
          print("=" * 70)
          print("PHASE 9: MoR (Mixture of Recursions)")
          print("=" * 70)
          
          def test_mor_config():
              from dfastllm.engine.mor_decoder import MoRConfig
              config = MoRConfig(
                  min_recursions=1,
                  max_recursions=4,
                  enabled=True
              )
              assert config.min_recursions == 1
              assert config.max_recursions == 4
              return {"enabled": config.enabled}
          
          def test_mor_stats():
              from dfastllm.engine.mor_decoder import MoRStats
              stats = MoRStats()
              stats.update(processed=10, skipped=2, easy=5, medium=3, hard=2, recursions=20)
              return {
                  "avg_recursions": stats.avg_recursions_per_token,
                  "total_processed": stats.total_tokens_processed
              }
          
          def test_mor_routing():
              from dfastllm.engine.mor_decoder import MoRConfig
              # Test that config parameters work correctly
              config = MoRConfig(
                  difficulty_threshold_low=0.9,
                  difficulty_threshold_high=0.5
              )
              assert config.difficulty_threshold_low > config.difficulty_threshold_high
              return {"routing_config_valid": True}
          
          suite.run_test("MoRConfig creation", "MoR", test_mor_config)
          suite.run_test("MoRStats tracking", "MoR", test_mor_stats)
          suite.run_test("MoR routing config", "MoR", test_mor_routing)
          
          # ============================================================
          # PHASE 10: APD (Adaptive Parallel Decoding) Tests
          # ============================================================
          print("")
          print("=" * 70)
          print("PHASE 10: APD (Adaptive Parallel Decoding)")
          print("=" * 70)
          
          def test_apd_config():
              from dfastllm.engine.apd import APDConfig
              config = APDConfig(
                  max_parallel_tokens=8,
                  acceptance_threshold=0.3,
                  enabled=True
              )
              assert config.max_parallel_tokens == 8
              return {"max_parallel": config.max_parallel_tokens}
          
          def test_apd_functionality():
              from dfastllm.engine.apd import APDConfig
              config = APDConfig()
              assert config.enabled == True
              assert config.dllm_weight == 1.0
              return {"dllm_weight": config.dllm_weight, "ar_weight": config.ar_weight}
          
          suite.run_test("APDConfig creation", "APD", test_apd_config)
          suite.run_test("APD functionality", "APD", test_apd_functionality)
          
          # ============================================================
          # PHASE 11: Diffusion Sampler Tests
          # ============================================================
          print("")
          print("=" * 70)
          print("PHASE 11: Diffusion Sampler")
          print("=" * 70)
          
          def test_sampling_params():
              from dfastllm.engine.sampling_params import SamplingParams
              params = SamplingParams(
                  max_tokens=100,
                  temperature=0.7,
                  top_p=0.9,
                  top_k=50
              )
              assert params.max_tokens == 100
              assert params.temperature == 0.7
              return {"params": "valid"}
          
          def test_diffusion_config():
              from dfastllm.config import DFastLLMConfig
              config = DFastLLMConfig(
                  model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                  diffusion_steps=64,
                  enable_apd=True,
                  enable_mor=True
              )
              assert config.diffusion_steps == 64
              return {"diffusion_steps": config.diffusion_steps}
          
          suite.run_test("SamplingParams creation", "Diffusion", test_sampling_params)
          suite.run_test("DFastLLMConfig creation", "Diffusion", test_diffusion_config)
          
          # ============================================================
          # FINAL SUMMARY
          # ============================================================
          print("")
          print("=" * 70)
          print("                    FINAL TEST SUMMARY")
          print("=" * 70)
          
          summary = suite.summary()
          
          print(f"""
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  COMPREHENSIVE LLM INFERENCE TEST RESULTS               â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  Total Tests:    {summary['total']:>3}                                    â”‚
          â”‚  Passed:         {summary['passed']:>3}  âœ…                                â”‚
          â”‚  Failed:         {summary['failed']:>3}  {'âŒ' if summary['failed'] > 0 else '  '}                                â”‚
          â”‚  Pass Rate:      {summary['pass_rate']:>5.1f}%                              â”‚
          â”‚  Duration:       {summary['duration_sec']:>5.1f}s                              â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          """)
          
          print("\nResults by Category:")
          print("â”€" * 40)
          for category, stats in summary['by_category'].items():
              total = stats['passed'] + stats['failed']
              pct = (stats['passed'] / total * 100) if total > 0 else 0
              status = "âœ…" if stats['failed'] == 0 else "âš ï¸"
              print(f"  {status} {category:20} {stats['passed']}/{total} ({pct:.0f}%)")
          
          print("")
          if summary['failed'] == 0:
              print("ğŸ‰ ALL TESTS PASSED - PRODUCTION READY!")
          else:
              print(f"âš ï¸  {summary['failed']} test(s) failed - review required")
              print("\nFailed tests:")
              for r in suite.results:
                  if not r.passed:
                      print(f"  âŒ {r.category}/{r.name}: {r.details[:80]}")
          
          print("")
          print("=" * 70)
          
          # Exit with appropriate code
          sys.exit(0 if summary['failed'] == 0 else 1)
          
          COMPREHENSIVE_TEST
          
          echo ""
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘     COMPREHENSIVE TEST SUITE COMPLETE                            â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
