# vdiff InferenceService for Red Hat OpenShift AI (RHOAI)
# Deploy this after the ServingRuntime is created
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llada-8b-instruct
  labels:
    # Required for RHOAI dashboard visibility
    opendatahub.io/dashboard: "true"
  annotations:
    # Use RawDeployment mode (recommended for RHOAI)
    serving.kserve.io/deploymentMode: "RawDeployment"
    # Enable route creation in OpenShift
    serving.knative.openshift.io/enablePassthrough: "true"
    # Sidecar injection (if using Service Mesh)
    sidecar.istio.io/inject: "true"
spec:
  predictor:
    # Replica configuration
    minReplicas: 1
    maxReplicas: 1
    
    # Request timeout (5 minutes for long generations)
    timeout: 300
    
    # Model specification
    model:
      # Model format
      modelFormat:
        name: diffusion-llm
      
      # Use vdiff runtime
      runtime: vdiff-runtime
      
      # Model location - use one of these options:
      # Option 1: HuggingFace model (downloaded at startup)
      # storageUri: "hf://GSAI-ML/LLaDA-8B-Instruct"
      
      # Option 2: S3/MinIO storage
      # storageUri: "s3://models/llada-8b-instruct"
      
      # Option 3: PVC with pre-downloaded model (recommended for RHOAI)
      storageUri: "pvc://model-pvc/llada-8b-instruct"
      
      # Resource requirements
      resources:
        requests:
          cpu: "4"
          memory: "16Gi"
          nvidia.com/gpu: "1"
        limits:
          cpu: "8"
          memory: "32Gi"
          nvidia.com/gpu: "1"
      
      # Environment variables
      env:
        - name: VDIFF_ENABLE_APD
          value: "true"
        - name: VDIFF_APD_MAX_PARALLEL
          value: "8"
        - name: VDIFF_APD_THRESHOLD
          value: "0.3"
        - name: VDIFF_MAX_MODEL_LEN
          value: "4096"
        - name: VDIFF_GPU_MEMORY_UTILIZATION
          value: "0.9"
    
    # Node selector for GPU nodes
    nodeSelector:
      nvidia.com/gpu.present: "true"
    
    # Tolerations for GPU nodes
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "odh-notebook"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"

---
# PersistentVolumeClaim for model storage (recommended for RHOAI)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-pvc
  labels:
    opendatahub.io/dashboard: "true"
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: gp3-csi  # Adjust for your OpenShift storage class
