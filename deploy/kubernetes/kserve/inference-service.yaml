# dfastllm InferenceService for KServe
# Deploy this after the ServingRuntime is created
# Works with: EKS, GKE, AKS, OpenShift, or self-managed Kubernetes
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llada-8b-instruct
  labels:
    serving.kserve.io/dashboard: "true"
  annotations:
    # Use RawDeployment mode for simpler deployment
    serving.kserve.io/deploymentMode: "RawDeployment"
    # Optional: Enable Istio sidecar if using Service Mesh
    # sidecar.istio.io/inject: "true"
spec:
  predictor:
    # Replica configuration
    minReplicas: 1
    maxReplicas: 1
    
    # Request timeout (5 minutes for long generations)
    timeout: 300
    
    # Model specification
    model:
      # Model format
      modelFormat:
        name: diffusion-llm
      
      # Use dfastllm runtime
      runtime: dfastllm-runtime
      
      # Model location - use one of these options:
      # Option 1: HuggingFace model (downloaded at startup)
      # storageUri: "hf://GSAI-ML/LLaDA-8B-Instruct"
      
      # Option 2: S3/MinIO storage
      # storageUri: "s3://models/llada-8b-instruct"
      
      # Option 3: PVC with pre-downloaded model (recommended for production)
      storageUri: "pvc://model-pvc/llada-8b-instruct"
      
      # Resource requirements
      resources:
        requests:
          cpu: "4"
          memory: "16Gi"
          nvidia.com/gpu: "1"
        limits:
          cpu: "8"
          memory: "32Gi"
          nvidia.com/gpu: "1"
      
      # Environment variables
      env:
        - name: DFASTLLM_ENABLE_APD
          value: "true"
        - name: DFASTLLM_APD_MAX_PARALLEL
          value: "8"
        - name: DFASTLLM_APD_THRESHOLD
          value: "0.3"
        - name: DFASTLLM_MAX_MODEL_LEN
          value: "4096"
        - name: DFASTLLM_GPU_MEMORY_UTILIZATION
          value: "0.9"
    
    # Node selector for GPU nodes
    nodeSelector:
      nvidia.com/gpu.present: "true"
    
    # Tolerations for GPU nodes
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"

---
# PersistentVolumeClaim for model storage (recommended for production)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  # Adjust storageClassName for your cluster:
  # - AWS EKS: gp3, gp2
  # - GCP GKE: standard, premium-rwo
  # - Azure AKS: managed-premium, default
  # - OpenShift: gp3-csi, managed-nfs-storage
  # storageClassName: gp3
