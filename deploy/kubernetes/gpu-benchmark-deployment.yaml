apiVersion: v1
kind: ConfigMap
metadata:
  name: dfastllm-gpu-config
data:
  VDIFF_MODEL: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  VDIFF_MAX_MODEL_LEN: "2048"
  VDIFF_TRUST_REMOTE_CODE: "true"
  VDIFF_PORT: "8000"
  VDIFF_DIFFUSION_STEPS: "64"
  VDIFF_ENABLE_APD: "true"
  VDIFF_HYBRID_ENABLED: "true"
  VDIFF_HYBRID_MODE: "deer"
  VDIFF_MOR_ENABLED: "true"
  VDIFF_COMPILE: "true"
  VDIFF_FLASH_ATTENTION: "true"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: dfastllm-gpu-benchmark
  labels:
    app: dfastllm-benchmark
spec:
  backoffLimit: 0
  template:
    metadata:
      labels:
        app: dfastllm-benchmark
    spec:
      restartPolicy: Never
      containers:
      - name: benchmark
        image: quay.io/modh/vllm:rhoai-2.20-cuda
        imagePullPolicy: Always
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          
          echo "================================================================"
          echo "   dfastllm GPU Benchmark & E2E Test Suite"
          echo "   Date: $(date)"
          echo "================================================================"
          echo ""
          
          # Check GPU
          echo "=== GPU Information ==="
          nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv
          echo ""
          
          # Install dfastllm
          echo "=== Installing dfastllm ==="
          pip install --no-cache-dir --no-deps --target=/tmp/dfastllm_pkg git+https://github.com/mwaykole/vdiff.git@main
          export PYTHONPATH="/tmp/dfastllm_pkg:$PYTHONPATH"
          echo "dfastllm installed successfully"
          echo ""
          
          # Run comprehensive tests
          python3 << 'PYTHON_SCRIPT'
          import sys
          import time
          import json
          import os
          
          sys.path.insert(0, '/tmp/dfastllm_pkg')
          
          print("=" * 70)
          print("PHASE 1: SOLID Architecture Tests")
          print("=" * 70)
          
          # Test 1: Base classes
          from dfastllm.engine.base import BaseStats, BaseConfig, BaseCache, EntropyComputer
          from dfastllm.engine.hybrid_engine import HybridStats, HybridConfig, HybridMode
          from dfastllm.engine.continuous_batching import BatcherStats, BatcherConfig, PrefixCache
          from dfastllm.engine.entropy_controller import EntropyStats, EntropyConfig
          
          tests_passed = 0
          tests_failed = 0
          
          # SOLID Tests
          try:
              assert isinstance(HybridStats(), BaseStats), "HybridStats should inherit BaseStats"
              assert isinstance(BatcherStats(), BaseStats), "BatcherStats should inherit BaseStats"
              assert isinstance(EntropyStats(), BaseStats), "EntropyStats should inherit BaseStats"
              print("âœ… Liskov Substitution: All Stats inherit BaseStats")
              tests_passed += 1
          except Exception as e:
              print(f"âŒ Liskov Substitution failed: {e}")
              tests_failed += 1
          
          try:
              assert isinstance(HybridConfig(), BaseConfig), "HybridConfig should inherit BaseConfig"
              assert isinstance(BatcherConfig(), BaseConfig), "BatcherConfig should inherit BaseConfig"
              assert isinstance(EntropyConfig(), BaseConfig), "EntropyConfig should inherit BaseConfig"
              print("âœ… Open/Closed: All Configs inherit BaseConfig")
              tests_passed += 1
          except Exception as e:
              print(f"âŒ Open/Closed failed: {e}")
              tests_failed += 1
          
          try:
              cache = PrefixCache(max_cache_size=10)
              assert isinstance(cache, BaseCache), "PrefixCache should inherit BaseCache"
              print("âœ… Interface Segregation: PrefixCache inherits BaseCache")
              tests_passed += 1
          except Exception as e:
              print(f"âŒ Interface Segregation failed: {e}")
              tests_failed += 1
          
          print()
          print("=" * 70)
          print("PHASE 2: PyTorch/CUDA Availability")
          print("=" * 70)
          
          try:
              import torch
              print(f"âœ… PyTorch version: {torch.__version__}")
              print(f"âœ… CUDA available: {torch.cuda.is_available()}")
              if torch.cuda.is_available():
                  print(f"âœ… CUDA device: {torch.cuda.get_device_name(0)}")
                  print(f"âœ… CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
              tests_passed += 1
          except Exception as e:
              print(f"âŒ PyTorch check failed: {e}")
              tests_failed += 1
          
          print()
          print("=" * 70)
          print("PHASE 3: Model Loading Test")
          print("=" * 70)
          
          try:
              from transformers import AutoTokenizer, AutoModelForCausalLM
              
              model_name = os.environ.get("VDIFF_MODEL", "TinyLlama/TinyLlama-1.1B-Chat-v1.0")
              print(f"Loading model: {model_name}")
              
              start = time.time()
              tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
              load_tokenizer_time = time.time() - start
              print(f"âœ… Tokenizer loaded in {load_tokenizer_time:.2f}s")
              
              start = time.time()
              model = AutoModelForCausalLM.from_pretrained(
                  model_name,
                  trust_remote_code=True,
                  torch_dtype=torch.float16,
                  device_map="auto"
              )
              load_model_time = time.time() - start
              print(f"âœ… Model loaded in {load_model_time:.2f}s")
              
              # Print model info
              total_params = sum(p.numel() for p in model.parameters())
              print(f"âœ… Model parameters: {total_params/1e9:.2f}B")
              tests_passed += 1
          except Exception as e:
              print(f"âŒ Model loading failed: {e}")
              tests_failed += 1
              model = None
              tokenizer = None
          
          print()
          print("=" * 70)
          print("PHASE 4: Generation Benchmark")
          print("=" * 70)
          
          if model is not None and tokenizer is not None:
              try:
                  prompts = [
                      "The quick brown fox",
                      "In the beginning",
                      "Once upon a time",
                      "The meaning of life is",
                      "Python is a programming language that"
                  ]
                  
                  total_tokens = 0
                  total_time = 0
                  
                  for i, prompt in enumerate(prompts):
                      inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
                      
                      start = time.time()
                      with torch.no_grad():
                          outputs = model.generate(
                              **inputs,
                              max_new_tokens=50,
                              do_sample=True,
                              temperature=0.7,
                              pad_token_id=tokenizer.eos_token_id
                          )
                      gen_time = time.time() - start
                      
                      new_tokens = outputs.shape[1] - inputs.input_ids.shape[1]
                      total_tokens += new_tokens
                      total_time += gen_time
                      
                      output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                      print(f"  Prompt {i+1}: {new_tokens} tokens in {gen_time:.3f}s ({new_tokens/gen_time:.1f} tok/s)")
                  
                  avg_tps = total_tokens / total_time
                  print(f"\nâœ… Average throughput: {avg_tps:.1f} tokens/second")
                  print(f"âœ… Total: {total_tokens} tokens in {total_time:.2f}s")
                  tests_passed += 1
              except Exception as e:
                  print(f"âŒ Generation benchmark failed: {e}")
                  tests_failed += 1
          
          print()
          print("=" * 70)
          print("PHASE 5: Hybrid Engine Component Tests")
          print("=" * 70)
          
          try:
              from dfastllm.engine.hybrid_engine import HybridStats, HybridConfig, HybridMode
              
              # Test HybridStats
              stats = HybridStats()
              stats.update(drafted=8, accepted=6, diffusion_time=0.01, ar_time=0.005)
              stats.update(drafted=10, accepted=8, diffusion_time=0.012, ar_time=0.006)
              
              assert stats.total_drafts == 2
              assert stats.tokens_accepted == 14
              assert stats.tokens_rejected == 4
              print(f"âœ… HybridStats: {stats.tokens_accepted} accepted, {stats.tokens_rejected} rejected")
              print(f"   Acceptance rate: {stats.draft_acceptance_rate:.2%}")
              
              # Test HybridConfig
              config = HybridConfig(mode=HybridMode.DEER, draft_block_size=8)
              assert config.mode == HybridMode.DEER
              assert config.draft_block_size == 8
              print(f"âœ… HybridConfig: mode={config.mode.value}, draft_size={config.draft_block_size}")
              
              tests_passed += 1
          except Exception as e:
              print(f"âŒ Hybrid engine tests failed: {e}")
              tests_failed += 1
          
          print()
          print("=" * 70)
          print("PHASE 6: Prefix Cache Performance")
          print("=" * 70)
          
          try:
              from dfastllm.engine.continuous_batching import PrefixCache
              
              cache = PrefixCache(max_cache_size=100, min_prefix_length=4)
              
              # Benchmark cache operations
              import time
              
              iterations = 1000
              tokens = list(range(20))
              
              # Put benchmark
              start = time.time()
              for i in range(iterations):
                  cache.put(list(range(i*10, i*10+20)), {"kv": i})
              put_time = (time.time() - start) * 1000
              
              # Get benchmark
              cache2 = PrefixCache(max_cache_size=100, min_prefix_length=4)
              cache2.put(tokens, {"kv": "test"})
              
              start = time.time()
              for _ in range(iterations):
                  cache2.get(tokens)
              get_time = (time.time() - start) * 1000
              
              print(f"âœ… Cache PUT: {put_time:.2f}ms for {iterations} ops ({put_time/iterations*1000:.2f}Î¼s/op)")
              print(f"âœ… Cache GET: {get_time:.2f}ms for {iterations} ops ({get_time/iterations*1000:.2f}Î¼s/op)")
              print(f"âœ… Hit rate: {cache2.hit_rate*100:.1f}%")
              
              tests_passed += 1
          except Exception as e:
              print(f"âŒ Prefix cache tests failed: {e}")
              tests_failed += 1
          
          print()
          print("=" * 70)
          print("PHASE 7: Entropy Controller Tests")
          print("=" * 70)
          
          try:
              from dfastllm.engine.entropy_controller import EntropyConfig, EntropyStats, AdaptationStrategy
              
              config = EntropyConfig(
                  strategy=AdaptationStrategy.COMBINED,
                  high_entropy_threshold=2.0,
                  low_entropy_threshold=0.5
              )
              assert config.strategy == AdaptationStrategy.COMBINED
              print(f"âœ… EntropyConfig: strategy={config.strategy.value}")
              
              stats = EntropyStats()
              stats.total_predictions = 100
              stats.high_entropy_count = 20
              stats.low_entropy_count = 60
              
              result = stats.to_dict()
              print(f"âœ… EntropyStats: high={result['high_entropy_pct']}%, low={result['low_entropy_pct']}%")
              
              tests_passed += 1
          except Exception as e:
              print(f"âŒ Entropy controller tests failed: {e}")
              tests_failed += 1
          
          print()
          print("=" * 70)
          print("                    FINAL RESULTS")
          print("=" * 70)
          total = tests_passed + tests_failed
          print(f"Tests Passed: {tests_passed}/{total}")
          print(f"Tests Failed: {tests_failed}/{total}")
          print(f"Pass Rate: {tests_passed/total*100:.1f}%")
          print()
          
          if tests_failed == 0:
              print("ðŸŽ‰ ALL GPU BENCHMARK TESTS PASSED!")
              print("âœ… PRODUCTION READY")
          else:
              print(f"âš ï¸  {tests_failed} tests failed - review required")
          
          print("=" * 70)
          
          # Exit with appropriate code
          sys.exit(0 if tests_failed == 0 else 1)
          PYTHON_SCRIPT
          
          echo ""
          echo "=== GPU Benchmark Complete ==="
        envFrom:
        - configMapRef:
            name: dfastllm-gpu-config
        resources:
          limits:
            cpu: "4"
            memory: "32Gi"
            nvidia.com/gpu: "1"
          requests:
            cpu: "2"
            memory: "16Gi"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: cache
          mountPath: /root/.cache
      volumes:
      - name: cache
        emptyDir: {}
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
