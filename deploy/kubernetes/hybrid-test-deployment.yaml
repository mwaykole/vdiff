apiVersion: v1
kind: Namespace
metadata:
  name: dfastllm-hybrid-test
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: dfastllm-hybrid-config
  namespace: dfastllm-hybrid-test
data:
  # Model configuration
  VDIFF_MODEL: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  VDIFF_MAX_MODEL_LEN: "2048"
  VDIFF_DTYPE: "auto"
  VDIFF_TRUST_REMOTE_CODE: "true"
  
  # Server configuration  
  VDIFF_HOST: "0.0.0.0"
  VDIFF_PORT: "8000"
  
  # Diffusion settings
  VDIFF_DIFFUSION_STEPS: "64"
  VDIFF_BLOCK_SIZE: "32"
  
  # APD settings
  VDIFF_ENABLE_APD: "true"
  VDIFF_APD_MAX_PARALLEL: "8"
  VDIFF_APD_THRESHOLD: "0.3"
  
  # === NEW: Hybrid Diffusion-AR Mode ===
  # Based on DEER paper: https://czc726.github.io/DEER/
  VDIFF_HYBRID_ENABLED: "true"
  VDIFF_HYBRID_MODE: "deer"
  VDIFF_HYBRID_DRAFT_SIZE: "8"
  VDIFF_HYBRID_MAX_DRAFT: "32"
  VDIFF_HYBRID_THRESHOLD: "0.3"
  VDIFF_HYBRID_ADAPTIVE: "true"
  
  # === NEW: MoR (Mixture of Recursions) ===
  VDIFF_MOR_ENABLED: "true"
  VDIFF_MOR_MIN_RECURSIONS: "1"
  VDIFF_MOR_MAX_RECURSIONS: "4"
  VDIFF_MOR_CONF_HIGH: "0.9"
  VDIFF_MOR_CONF_LOW: "0.5"
  VDIFF_MOR_STRATEGY: "confidence"
  
  # Performance optimizations
  VDIFF_COMPILE: "true"
  VDIFF_COMPILE_MODE: "reduce-overhead"
  VDIFF_FLASH_ATTENTION: "true"
  VDIFF_MIXED_PRECISION: "true"
  VDIFF_ADAPTIVE_STEPS: "true"
  VDIFF_CONFIDENCE_THRESHOLD: "0.95"
  VDIFF_EARLY_STOPPING: "true"
  
  # Resource limits
  VDIFF_MAX_CONCURRENT: "4"
  VDIFF_MAX_QUEUE_SIZE: "256"
  VDIFF_REQUEST_TIMEOUT: "300"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dfastllm-hybrid-server
  namespace: dfastllm-hybrid-test
  labels:
    app: dfastllm-hybrid
    version: v2.2.0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dfastllm-hybrid
  template:
    metadata:
      labels:
        app: dfastllm-hybrid
        version: v2.2.0
    spec:
      containers:
      - name: dfastllm
        image: quay.io/modh/vllm:rhoai-2.20-cuda
        imagePullPolicy: Always
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          echo "=============================================="
          echo "   dfastllm Hybrid Mode Cluster Test v2.2.0"
          echo "=============================================="
          
          echo ""
          echo "=== Installing dfastllm from source ==="
          pip install --no-cache-dir --no-deps --target=/tmp/dfastllm_pkg git+https://github.com/mwaykole/vdiff.git@main
          export PYTHONPATH="/tmp/dfastllm_pkg:$PYTHONPATH"
          export HF_HOME="/root/.cache/huggingface"
          export TRANSFORMERS_OFFLINE=0
          export HF_HUB_OFFLINE=0
          
          echo ""
          echo "=== Running Comprehensive Hybrid Mode Tests ==="
          python -c "
          import os
          import time
          import logging
          logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
          logger = logging.getLogger(__name__)

          print('=' * 60)
          print('  dfastllm Hybrid Mode - Cluster Test Suite')
          print('=' * 60)
          print()

          # Test 1: Import all new modules
          print('Test 1: Importing new modules...')
          try:
              from dfastllm.engine import (
                  DFastLLMEngine, SamplingParams,
                  HybridEngine, HybridConfig, HybridMode, HybridStats,
                  ContinuousBatchingScheduler, BatcherConfig, PrefixCache,
                  EntropyAdaptiveController, EntropyConfig,
              )
              print('  âœ“ All imports successful')
          except Exception as e:
              print(f'  âœ— Import failed: {e}')
              raise

          # Test 2: Check configuration
          print()
          print('Test 2: Configuration check...')
          from dfastllm.config import DFastLLMConfig
          config = DFastLLMConfig.from_env()
          
          print(f'  Model: {config.model}')
          print(f'  Hybrid Mode: {config.enable_hybrid}')
          print(f'  Hybrid Strategy: {config.hybrid_mode}')
          print(f'  MoR Enabled: {config.enable_mor}')
          print(f'  torch.compile: {config.compile_model}')
          print(f'  Flash Attention: {config.use_flash_attention}')
          print('  âœ“ Configuration loaded')

          # Test 3: Initialize engine
          print()
          print('Test 3: Engine initialization...')
          start = time.time()
          engine = DFastLLMEngine(config)
          init_time = time.time() - start
          print(f'  Init time: {init_time:.2f}s')
          print(f'  Device: {engine._device}')
          print(f'  Is diffusion model: {engine._is_diffusion_model}')
          print('  âœ“ Engine initialized')

          # Test 4: Health check
          print()
          print('Test 4: Health check...')
          health = engine.get_health()
          print(f'  Status: {health.status}')
          print(f'  State: {health.state.value}')
          print(f'  Model loaded: {health.model_loaded}')
          if health.gpu_memory_total_mb > 0:
              print(f'  GPU Memory: {health.gpu_memory_used_mb:.0f}/{health.gpu_memory_total_mb:.0f} MB')
          print('  âœ“ Health check passed')

          # Test 5: Generation tests
          print()
          print('Test 5: Generation tests...')
          prompts = [
              'Hello, how are you today?',
              'Explain quantum computing in simple terms.',
              'Write a haiku about the ocean.',
          ]
          
          results = []
          for i, prompt in enumerate(prompts):
              params = SamplingParams(max_tokens=32, temperature=0.7)
              start = time.time()
              output = engine.generate(prompt, params)
              latency = (time.time() - start) * 1000
              
              tokens = output.outputs[0].token_ids
              text = output.outputs[0].text[:50] + '...' if len(output.outputs[0].text) > 50 else output.outputs[0].text
              tokens_per_sec = len(tokens) / (latency / 1000) if latency > 0 else 0
              
              results.append({
                  'latency': latency,
                  'tokens': len(tokens),
                  'tps': tokens_per_sec,
              })
              
              print(f'  [{i+1}] {latency:.0f}ms, {len(tokens)} tokens, {tokens_per_sec:.1f} tok/s')
              print(f'      Output: {text}')
          
          avg_latency = sum(r['latency'] for r in results) / len(results)
          avg_tps = sum(r['tps'] for r in results) / len(results)
          print(f'  Average: {avg_latency:.0f}ms, {avg_tps:.1f} tok/s')
          print('  âœ“ Generation tests passed')

          # Test 6: Entropy controller
          print()
          print('Test 6: Entropy-adaptive controller...')
          controller = EntropyAdaptiveController(EntropyConfig(enabled=True))
          print(f'  Strategy: {controller.config.strategy.value}')
          print(f'  Draft length: {controller.get_draft_length()}')
          print(f'  Diffusion steps: {controller.get_diffusion_steps()}')
          print('  âœ“ Entropy controller working')

          # Test 7: Prefix cache
          print()
          print('Test 7: Prefix cache...')
          cache = PrefixCache(max_cache_size=100)
          test_tokens = list(range(100))
          cache.put(test_tokens, {'kv': 'test_data'})
          hit = cache.get(test_tokens)
          miss = cache.get(list(range(200, 300)))
          print(f'  Cache hit: {hit is not None}')
          print(f'  Cache miss: {miss is None}')
          print(f'  Hit rate: {cache.get_hit_rate():.2%}')
          print('  âœ“ Prefix cache working')

          # Test 8: Engine stats
          print()
          print('Test 8: Engine statistics...')
          stats = engine.get_stats()
          print(f'  Requests processed: {stats.get(\"requests_processed\", 0)}')
          print(f'  Tokens generated: {stats.get(\"tokens_generated\", 0)}')
          print(f'  Avg latency: {stats.get(\"avg_latency_ms\", 0):.0f}ms')
          if 'hybrid' in stats:
              print(f'  Hybrid stats: {stats[\"hybrid\"]}')
          print('  âœ“ Stats collected')

          # Summary
          print()
          print('=' * 60)
          print('  TEST SUMMARY')
          print('=' * 60)
          print(f'  Total tests: 8')
          print(f'  All passed: âœ“')
          print(f'  Average latency: {avg_latency:.0f}ms')
          print(f'  Average throughput: {avg_tps:.1f} tok/s')
          print(f'  Hybrid mode: {config.hybrid_mode}')
          print(f'  MoR enabled: {config.enable_mor}')
          print('=' * 60)
          print()
          print('ðŸŽ‰ All Hybrid Mode Tests Passed!')
          print()
          "
          
          # Keep container running for additional tests if needed
          echo ""
          echo "=== Starting API Server for external tests ==="
          python -m dfastllm.entrypoints.launcher \
            --host 0.0.0.0 \
            --port 8000
        envFrom:
        - configMapRef:
            name: dfastllm-hybrid-config
        ports:
        - containerPort: 8000
          name: http
        resources:
          requests:
            memory: "8Gi"
            cpu: "2"
            nvidia.com/gpu: "1"
          limits:
            memory: "16Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 180
          periodSeconds: 30
          timeoutSeconds: 10
        volumeMounts:
        - name: cache-volume
          mountPath: /root/.cache
        - name: shm
          mountPath: /dev/shm
      volumes:
      - name: cache-volume
        emptyDir:
          sizeLimit: 20Gi
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 2Gi
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      nodeSelector:
        nvidia.com/gpu.present: "true"
---
apiVersion: v1
kind: Service
metadata:
  name: dfastllm-hybrid-service
  namespace: dfastllm-hybrid-test
  labels:
    app: dfastllm-hybrid
spec:
  type: ClusterIP
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: http
  selector:
    app: dfastllm-hybrid
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: dfastllm-hybrid-route
  namespace: dfastllm-hybrid-test
spec:
  to:
    kind: Service
    name: dfastllm-hybrid-service
  port:
    targetPort: http
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
