apiVersion: v1
kind: Namespace
metadata:
  name: vdiff-experiments
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: vdiff-config
  namespace: vdiff-experiments
data:
  # Model configuration
  VDIFF_MODEL: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  VDIFF_MAX_MODEL_LEN: "2048"
  VDIFF_DTYPE: "auto"
  VDIFF_TRUST_REMOTE_CODE: "true"
  
  # Server configuration  
  VDIFF_HOST: "0.0.0.0"
  VDIFF_PORT: "8000"
  
  # Diffusion settings
  VDIFF_DIFFUSION_STEPS: "64"
  VDIFF_BLOCK_SIZE: "32"
  
  # APD settings
  VDIFF_ENABLE_APD: "true"
  VDIFF_APD_MAX_PARALLEL: "8"
  VDIFF_APD_THRESHOLD: "0.3"
  
  # Performance optimizations (HIGH/MEDIUM priority)
  VDIFF_COMPILE: "true"
  VDIFF_COMPILE_MODE: "reduce-overhead"
  VDIFF_FLASH_ATTENTION: "true"
  VDIFF_MIXED_PRECISION: "true"
  VDIFF_ADAPTIVE_STEPS: "true"
  VDIFF_CONFIDENCE_THRESHOLD: "0.95"
  VDIFF_EARLY_STOPPING: "true"
  VDIFF_ATTENTION_CACHE: "false"
  
  # Resource limits
  VDIFF_MAX_CONCURRENT: "4"
  VDIFF_MAX_QUEUE_SIZE: "256"
  VDIFF_REQUEST_TIMEOUT: "300"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vdiff-server
  namespace: vdiff-experiments
  labels:
    app: vdiff
    version: v1.0.0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vdiff
  template:
    metadata:
      labels:
        app: vdiff
        version: v1.0.0
    spec:
      containers:
      - name: vdiff
        image: quay.io/modh/vllm:rhoai-2.20-cuda
        imagePullPolicy: Always
        command: ["/bin/bash", "-c"]
        args:
        - |
          echo "=== Installing vdiff from source ==="
          pip install --no-cache-dir git+https://github.com/mwaykole/vdiff.git@main
          
          echo "=== Starting vdiff server with optimizations ==="
          python -c "
          import os
          import logging
          logging.basicConfig(level=logging.INFO)
          
          from dfastllm.config import DFastLLMConfig
          from dfastllm.engine import DFastLLMEngine
          
          # Log optimization settings
          print('=== vdiff Optimization Settings ===')
          print(f'torch.compile: {os.getenv(\"VDIFF_COMPILE\", \"true\")}')
          print(f'Flash Attention: {os.getenv(\"VDIFF_FLASH_ATTENTION\", \"true\")}')
          print(f'Mixed Precision: {os.getenv(\"VDIFF_MIXED_PRECISION\", \"true\")}')
          print(f'Adaptive Steps: {os.getenv(\"VDIFF_ADAPTIVE_STEPS\", \"true\")}')
          print(f'Early Stopping: {os.getenv(\"VDIFF_EARLY_STOPPING\", \"true\")}')
          
          # Create config from environment
          config = DFastLLMConfig.from_env()
          print(f'Model: {config.model}')
          print(f'Device: auto-detected')
          
          # Initialize engine
          print('=== Initializing Engine ===')
          engine = DFastLLMEngine(config)
          
          # Run health check
          health = engine.health_check()
          print(f'Health Status: {health.status}')
          print(f'Model Loaded: {health.model_loaded}')
          print(f'Device: {health.device}')
          
          # Run a test generation
          print('=== Running Test Generation ===')
          from dfastllm.engine import SamplingParams
          params = SamplingParams(max_tokens=32, temperature=0.7)
          output = engine.generate('Hello, how are you?', params)
          print(f'Generated: {output.outputs[0].text}')
          print(f'Tokens: {output.outputs[0].token_ids}')
          print(f'Latency: {output.metrics.latency_ms:.2f}ms')
          
          print('=== All Tests Passed! ===')
          "
        envFrom:
        - configMapRef:
            name: vdiff-config
        resources:
          requests:
            memory: "8Gi"
            cpu: "2"
            nvidia.com/gpu: "1"
          limits:
            memory: "16Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: cache-volume
          mountPath: /root/.cache
        - name: shm
          mountPath: /dev/shm
      volumes:
      - name: cache-volume
        emptyDir:
          sizeLimit: 20Gi
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 2Gi
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      nodeSelector:
        nvidia.com/gpu.present: "true"
---
apiVersion: v1
kind: Service
metadata:
  name: vdiff-service
  namespace: vdiff-experiments
  labels:
    app: vdiff
spec:
  type: ClusterIP
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: http
  selector:
    app: vdiff
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: vdiff-route
  namespace: vdiff-experiments
spec:
  to:
    kind: Service
    name: vdiff-service
  port:
    targetPort: http
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect

